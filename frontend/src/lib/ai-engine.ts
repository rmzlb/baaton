/**
 * Baaton AI Engine v2 â€” Vercel AI SDK (@ai-sdk/google)
 *
 * Migrated from @google/generative-ai to Vercel AI SDK for:
 * - Built-in agentic loop (maxSteps)
 * - Typed tool results
 * - Unified provider interface (swap Gemini/Claude/GPT)
 * - Proper token usage from response metadata
 * - Automatic retry handling
 *
 * Keeps all existing features:
 * - State machine integration (ai-state.ts)
 * - Tool masking (5 contexts via ai-skills.ts)
 * - Conversation summarization
 * - Rate limiting + token budget
 * - 5-block Manus system prompt
 */

import { generateText, jsonSchema, type CoreMessage, type CoreTool } from 'ai';
import { createGoogleGenerativeAI } from '@ai-sdk/google';
import type { Issue, Project, Milestone } from './types';
import { getToolsForContext, type SkillContext } from './ai-skills';
import { executeSkill } from './ai-executor';
import type { SkillResult } from './ai-skills';
import {
  type AIStateContext,
  createInitialState,
  transition,
  stateToSkillContext,
  checkRateLimit,
  estimateTokens,
  checkBudget,
  summarizeHistory,
} from './ai-state';

// â”€â”€â”€ API Key (fetched from backend) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
let _cachedApiKey: string | null = null;
const API_URL = import.meta.env.VITE_API_URL || '';

async function getGeminiApiKey(authToken?: string): Promise<string> {
  if (_cachedApiKey) return _cachedApiKey;
  if (API_URL && authToken) {
    try {
      const res = await fetch(`${API_URL}/api/v1/ai/key`, {
        headers: { Authorization: `Bearer ${authToken}` },
        signal: AbortSignal.timeout(3000),
      });
      if (res.ok) {
        const data = await res.json();
        if (data.key) {
          _cachedApiKey = data.key;
          return _cachedApiKey;
        }
      }
    } catch { /* backend unreachable */ }
  }
  throw new Error('AI non configurÃ©. ClÃ© API manquante.');
}

// â”€â”€â”€ Errors â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export class RateLimitError extends Error {
  constructor() {
    super('Rate limit exceeded â€” please wait a moment before sending another message.');
    this.name = 'RateLimitError';
  }
}

// â”€â”€â”€ Convert Gemini Tool Declarations â†’ AI SDK Tools â”€â”€â”€â”€
// Bridge: getToolsForContext returns Gemini format [{functionDeclarations: [...]}]
// AI SDK needs Record<string, CoreTool> with jsonSchema parameters

function convertGeminiPropertyToJsonSchema(prop: any): any {
  if (!prop) return { type: 'string' };
  const schema: any = {};
  switch (prop.type) {
    case 'STRING':
      schema.type = 'string';
      break;
    case 'NUMBER':
      schema.type = 'number';
      break;
    case 'BOOLEAN':
      schema.type = 'boolean';
      break;
    case 'ARRAY':
      schema.type = 'array';
      if (prop.items) {
        schema.items = convertGeminiPropertyToJsonSchema(prop.items);
      }
      break;
    case 'OBJECT':
      schema.type = 'object';
      if (prop.properties) {
        schema.properties = {};
        for (const [k, v] of Object.entries(prop.properties)) {
          schema.properties[k] = convertGeminiPropertyToJsonSchema(v);
        }
      }
      if (prop.required) schema.required = prop.required;
      break;
    default:
      schema.type = 'string';
  }
  if (prop.description) schema.description = prop.description;
  return schema;
}

function buildAISDKTools(
  skillContext: SkillContext,
  executor: (name: string, args: Record<string, unknown>) => Promise<any>,
): Record<string, CoreTool> {
  const geminiTools = getToolsForContext(skillContext);
  const declarations = geminiTools.flatMap((t: any) => t.functionDeclarations || []);
  const tools: Record<string, CoreTool> = {};

  for (const decl of declarations) {
    // Convert Gemini params â†’ JSON Schema
    const params = decl.parameters || { type: 'OBJECT', properties: {} };
    const schema = convertGeminiPropertyToJsonSchema(params);

    // Gemini API requires root schema to be { type: "object" } with properties
    // Ensure the root always has type "object" and properties defined
    if (schema.type !== 'object') {
      schema.type = 'object';
    }
    if (!schema.properties) {
      schema.properties = {};
    }

    tools[decl.name] = {
      description: decl.description,
      parameters: jsonSchema(schema),
      execute: async (args: any) => executor(decl.name, args),
    };
  }

  return tools;
}

// â”€â”€â”€ Context Builder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

export function buildProjectContext(projects: Project[], allIssues: Record<string, Issue[]>): string {
  const lines: string[] = ['# Current Project Data\n'];

  for (const project of projects) {
    const issues = allIssues[project.id] || [];
    if (issues.length === 0) continue;

    const statusCounts: Record<string, number> = {};
    const priorityCounts: Record<string, number> = {};
    const typeCounts: Record<string, number> = {};
    const categoryCounts: Record<string, number> = {};

    for (const issue of issues) {
      statusCounts[issue.status] = (statusCounts[issue.status] || 0) + 1;
      if (issue.priority) priorityCounts[issue.priority] = (priorityCounts[issue.priority] || 0) + 1;
      typeCounts[issue.type] = (typeCounts[issue.type] || 0) + 1;
      for (const cat of issue.category || []) {
        categoryCounts[cat] = (categoryCounts[cat] || 0) + 1;
      }
    }

    lines.push(`## ${project.prefix} â€” ${project.name} (ID: ${project.id})`);
    lines.push(`Total: ${issues.length} issues`);
    lines.push(`Status: ${Object.entries(statusCounts).map(([k, v]) => `${k}=${v}`).join(', ')}`);
    if (Object.keys(priorityCounts).length > 0) {
      lines.push(`Priority: ${Object.entries(priorityCounts).map(([k, v]) => `${k}=${v}`).join(', ')}`);
    }
    lines.push(`Types: ${Object.entries(typeCounts).map(([k, v]) => `${k}=${v}`).join(', ')}`);
    if (Object.keys(categoryCounts).length > 0) {
      lines.push(`Domains: ${Object.entries(categoryCounts).map(([k, v]) => `${k}=${v}`).join(', ')}`);
    }

    const open = issues.filter((i) => i.status === 'todo' || i.status === 'in_progress' || i.status === 'in_review');
    if (open.length > 0) {
      lines.push(`\nOpen issues:`);
      for (const i of open.slice(0, 30)) {
        const prio = i.priority ? ` [${i.priority}]` : '';
        const cats = (i.category || []).length > 0 ? ` {${(i.category || []).join(',')}}` : '';
        const tags = i.tags.length > 0 ? ` #${i.tags.join(' #')}` : '';
        const type = ` (${i.type})`;
        lines.push(`- ${i.display_id} (uuid:${i.id}) | ${i.status}${prio}${type}${cats}${tags} | ${i.title}`);
      }
      if (open.length > 30) lines.push(`  ... and ${open.length - 30} more`);
    }

    const done = issues
      .filter((i) => i.status === 'done')
      .sort((a, b) => new Date(b.updated_at).getTime() - new Date(a.updated_at).getTime());
    if (done.length > 0) {
      lines.push(`\nRecently done (${done.length} total):`);
      for (const i of done.slice(0, 10)) {
        lines.push(`- ${i.display_id} | ${i.title}`);
      }
    }

    lines.push('');
  }

  return lines.join('\n');
}

// â”€â”€â”€ System Prompt (5-Block Manus Pattern) â”€â”€â”€â”€

export function buildSystemPrompt(context: string): string {
  const STATIC_BLOCKS = `# BLOCK 1 â€” IDENTITY

Tu es **Baaton AI**, le copilote intÃ©grÃ© Ã  Baaton â€” un board d'orchestration pour agents IA de dÃ©veloppement.
Tes utilisateurs sont des **dÃ©veloppeurs et tech leads** qui utilisent des agents IA (Claude, GPT, Copilot, OpenClaw) pour coder.
Tu comprends : architecture logicielle, sprints agile, dette technique, CI/CD, code review, et orchestration d'agents.

## Contexte Baaton
Baaton = "You orchestrate. AI executes." C'est un Kanban/projet tracker spÃ©cialisÃ© pour :
- Suivre les issues de code (bugs, features, improvements) par projet
- Planifier des sprints et milestones pour des Ã©quipes dev + agents IA
- CatÃ©goriser par domaine technique : FRONT, BACK, API, DB, INFRA, UX, DEVOPS
- GÃ©rer des tags colorÃ©s pour le contexte (ex: "ElevenLabs", "Auth", "Perf")
- Connecter des agents IA (OpenClaw) pour automatiser le triage et l'exÃ©cution

## Ton RÃ´le
Tu es le PM assistant de l'Ã©quipe. Tu ne codes pas, mais tu :
- Tries et priorises les issues intelligemment
- Proposes des milestones rÃ©alistes basÃ©s sur la vÃ©locitÃ© et les dÃ©pendances techniques
- DÃ©tectes les blockers, la dette technique, et les risques
- GÃ©nÃ¨res des PRD structurÃ©s avec specs techniques
- Comprends les catÃ©gories FRONT/BACK/API/DB et groupes les issues par domaine

# BLOCK 2 â€” SKILLS & CAPACITÃ‰S

## Tes Skills (fonctions exÃ©cutables) :

### ğŸ“‹ Lecture & Analyse
- **search_issues** â€” Chercher/filtrer des issues (texte, status, prioritÃ©, catÃ©gorie, projet)
- **get_project_metrics** â€” MÃ©triques dÃ©taillÃ©es (vÃ©locitÃ©, taux de complÃ©tion, distribution)
- **analyze_sprint** â€” Analyse de sprint, vÃ©locitÃ©, recommandations

### âœï¸ Actions
- **create_issue** â€” CrÃ©er une issue (titre, description, type, prioritÃ©, tags, catÃ©gorie)
- **update_issue** â€” Modifier une issue
- **bulk_update_issues** â€” Modifier N issues d'un coup
- **add_comment** â€” Ajouter un commentaire / note sur une issue

### ğŸ“„ GÃ©nÃ©ration
- **generate_prd** â€” GÃ©nÃ©rer un PRD structurÃ©

### ğŸ¯ Milestone Planning
- **plan_milestones** â€” Auto-group open issues into milestones (propose first, user confirms)
- **create_milestones_batch** â€” Create milestones after confirmation
- **adjust_timeline** â€” Adjust timeline based on new constraint/deadline

## RÃ¨gles d'ExÃ©cution
1. **TOUJOURS utiliser tes skills** pour accÃ©der aux donnÃ©es â€” jamais d'hallucination
2. **Actions directes** : crÃ©er, modifier, commenter â†’ exÃ©cute immÃ©diatement sans demander confirmation
3. **Actions destructives** (suppression) â†’ demande confirmation avant
4. **Bulk updates** â†’ liste les changements AVANT d'exÃ©cuter
5. **Cite les display_id** (ex: HLM-42) quand tu mentionnes des issues
6. **Pour update/bulk** â†’ utilise l'UUID (pas le display_id)
7. **RÃ©solution de projet** : quand l'utilisateur dit un nom ("helmai", "sqare"), matche avec le prefix

## Format de Sortie
- IDs exacts : UUID pour update/delete, display_id pour citation
- AprÃ¨s chaque action, confirme avec le rÃ©sultat (display_id + changement)
- Listes > 10 items : rÃ©sumÃ© + top 5 en dÃ©tail
- Ne rÃ©ponds JAMAIS avec des donnÃ©es que tu n'as pas obtenues via un skill

## Milestone Planning Flow
1. **plan_milestones** â†’ retourne proposed_milestones groupÃ©s avec target_dates
2. PrÃ©sente le plan formatÃ© au user
3. Demande confirmation
4. Sur confirmation â†’ **create_milestones_batch** avec les donnÃ©es exactes du plan
5. Ne rappelle PAS plan_milestones sur confirmation

## Issue Creation
1. Si projet ambigu â†’ demande
2. Remplis un max automatiquement (type, prioritÃ©, catÃ©gorie, tags)
3. NE DEMANDE PAS de confirmation â€” crÃ©e directement
4. AprÃ¨s : propose d'ajouter des images via âŒ˜V ou drag & drop

# BLOCK 3 â€” COMMUNICATION

- RÃ©ponds dans la langue de l'utilisateur (FR si franÃ§ais, EN si anglais)
- Parle comme un tech lead, pas comme un PM corporate
- Concis, actionnable, Markdown. Pas de bullshit, pas de fluff.
- Bullet points > paragraphes
- MÃ©triques concrÃ¨tes + pourcentages
- \`backticks\` pour les termes techniques
- Flag les blockers et la dette technique proactivement
- Emojis : âœ… done, ğŸ”„ in progress, ğŸ“‹ todo, ğŸš¨ urgent, â¸ï¸ backlog, ğŸ› bug, âœ¨ feature`;

  const DYNAMIC_BLOCKS = `# BLOCK 4 â€” DONNÃ‰ES PROJET (DYNAMIQUE)

${context}

# BLOCK 5 â€” OBJECTIFS ACTUELS

Aide l'utilisateur Ã  Ãªtre productif. ExÃ©cute efficacement. Propose des insights (bottlenecks, prioritÃ©s mal calibrÃ©es). Sois proactif.`;

  return `${STATIC_BLOCKS}\n\n${DYNAMIC_BLOCKS}`;
}

// â”€â”€â”€ Types â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

export interface AIResponse {
  text: string;
  skillsExecuted: SkillResult[];
  usage: {
    inputTokens: number;
    outputTokens: number;
    totalTokens: number;
    turnCount: number;
  };
  stateContext: AIStateContext;
}

type ApiClientType = {
  issues: {
    listByProject: (id: string, params?: Record<string, unknown>) => Promise<Issue[]>;
    create: (body: Record<string, unknown>) => Promise<Issue>;
    update: (id: string, body: Record<string, unknown>) => Promise<Issue>;
    delete: (id: string) => Promise<void>;
  };
  comments: {
    create: (issueId: string, body: { content: string; author_name: string }) => Promise<unknown>;
  };
  projects: {
    list: () => Promise<Project[]>;
  };
  milestones: {
    listByProject: (projectId: string) => Promise<Milestone[]>;
    create: (projectId: string, body: { name: string; description?: string; target_date?: string; status?: string }) => Promise<Milestone>;
    update: (id: string, body: Partial<Pick<Milestone, 'name' | 'description' | 'target_date' | 'status'>>) => Promise<Milestone>;
    delete: (id: string) => Promise<void>;
  };
};

// â”€â”€â”€ Main Generate Function (Vercel AI SDK) â”€â”€â”€

export async function generateAIResponse(
  userMessage: string,
  projects: Project[],
  allIssuesByProject: Record<string, Issue[]>,
  conversationHistory: { role: string; content: string }[],
  apiClient: ApiClientType,
  stateContext?: AIStateContext,
  authToken?: string,
): Promise<AIResponse> {
  // â”€â”€ Rate Limiting â”€â”€
  const rateCheck = checkRateLimit();
  if (!rateCheck.allowed) {
    throw new RateLimitError();
  }

  // â”€â”€ State Machine: init or use provided â”€â”€
  let state = stateContext ? { ...stateContext } : createInitialState();
  state = transition(state, { type: 'USER_MESSAGE', tokens: estimateTokens(userMessage) });

  // â”€â”€ Budget check â”€â”€
  const budget = checkBudget(state);
  if (!budget.ok) {
    return {
      text: `âš ï¸ ${budget.warning}`,
      skillsExecuted: [],
      usage: { inputTokens: 0, outputTokens: 0, totalTokens: 0, turnCount: state.usage.turnCount },
      stateContext: state,
    };
  }

  // â”€â”€ Get API key & create provider â”€â”€
  const apiKey = await getGeminiApiKey(authToken);
  const google = createGoogleGenerativeAI({ apiKey });

  // â”€â”€ Build context & prompt â”€â”€
  const context = buildProjectContext(projects, allIssuesByProject);
  const systemPrompt = buildSystemPrompt(context);

  // â”€â”€ Conversation summarization â”€â”€
  const optimizedHistory = summarizeHistory(conversationHistory);

  // â”€â”€ Build messages â”€â”€
  const messages: CoreMessage[] = optimizedHistory.slice(-8).map((m) => ({
    role: (m.role === 'user' ? 'user' : 'assistant') as 'user' | 'assistant',
    content: m.content,
  }));
  messages.push({ role: 'user', content: userMessage });

  // â”€â”€ Tool masking from state â”€â”€
  const skillContext = stateToSkillContext(state);
  const skillsExecuted: SkillResult[] = [];

  // â”€â”€ Build tools with executor bridge â”€â”€
  const executor = async (name: string, args: Record<string, unknown>) => {
    state = transition(state, { type: 'SKILL_STARTED', name });
    const startTime = performance.now();
    const result = await executeSkill(name, args, apiClient, allIssuesByProject, projects);
    const executionTime = Math.round(performance.now() - startTime);
    const enriched = { ...result, executionTimeMs: executionTime };
    skillsExecuted.push(enriched);

    if (result.success) {
      state = transition(state, { type: 'SKILL_COMPLETED', name, data: result.data });
    } else {
      state = transition(state, { type: 'SKILL_FAILED', name, error: result.error || 'Unknown error' });
    }

    return result.data || { success: result.success, error: result.error };
  };

  const tools = buildAISDKTools(skillContext, executor);

  try {
    // â”€â”€ Vercel AI SDK: generateText with tools + maxSteps â”€â”€
    // maxSteps = 5 â†’ agentic loop (same as our old 5-round loop)
    // The SDK handles: tool call â†’ execute â†’ feed result â†’ get next response
    const result = await generateText({
      model: google('gemini-2.0-flash'),
      system: systemPrompt,
      messages,
      tools,
      maxSteps: 5,
      temperature: 0.4,
      maxTokens: 2000,
    });

    // â”€â”€ Track usage from response metadata â”€â”€
    const usage = result.usage || { promptTokens: 0, completionTokens: 0, totalTokens: 0 };
    const inputTokens = usage.promptTokens || estimateTokens(systemPrompt + userMessage);
    const outputTokens = usage.completionTokens || estimateTokens(result.text || '');

    state = transition(state, { type: 'AI_RESPONSE', tokens: outputTokens });

    return {
      text: result.text || "Je n'ai pas pu gÃ©nÃ©rer de rÃ©ponse. RÃ©essaie avec plus de dÃ©tails.",
      skillsExecuted,
      usage: {
        inputTokens,
        outputTokens,
        totalTokens: inputTokens + outputTokens,
        turnCount: state.usage.turnCount,
      },
      stateContext: state,
    };
  } catch (err: any) {
    state = transition(state, { type: 'ERROR', error: String(err) });

    // â”€â”€ Typed error handling â”€â”€
    const msg = err?.message || String(err);
    if (msg.includes('429') || msg.includes('RESOURCE_EXHAUSTED')) {
      throw new RateLimitError();
    }
    if (msg.includes('403') || msg.includes('API_KEY')) {
      throw new Error('ClÃ© API invalide ou expirÃ©e. VÃ©rifie ta configuration.');
    }
    if (msg.includes('ECONNRESET') || msg.includes('fetch') || msg.includes('network')) {
      throw new Error('Erreur rÃ©seau. VÃ©rifie ta connexion et rÃ©essaie.');
    }
    throw err;
  }
}
